{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8f148b8-f294-45ca-bad1-419e6139b721",
   "metadata": {},
   "source": [
    "# Health Expenditure Predictor Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e772a5e1-03c6-4ecf-a66e-272f787f5f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lesga\\anaconda3\\envs\\Insurance\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.SubLayers import MultiHeadAttention\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1710ade6-21ed-4843-8f37-5b0a1f5ba77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HealthExpenditureModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Health Expenditures Module\n",
    "    \n",
    "    Args:\n",
    "        d_out (int): prediction output dimension\n",
    "        d_demo (int): demographic embedding dimesion if it exists, or 0\n",
    "        d_model (int): (multihead attention) model dimension\n",
    "        n_head (int): the number of (multihead attention) heads\n",
    "        dropout (float): dropout ratio of a multihead attention and a final linear layer\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, d_out, d_demo=0, d_model=512, n_head=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_demo = d_demo\n",
    "        \n",
    "        assert d_model % n_head == 0\n",
    "        self.multihead_attn = MultiHeadAttention(n_head, d_model, d_k = (d_model // n_head), d_v = (d_model // n_head), dropout=dropout)\n",
    "        \n",
    "        d_inter = d_model * 2 + d_demo\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(d_inter, d_inter // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_inter // 2, d_out)\n",
    "        )\n",
    "    \n",
    "    def forward(self, q, kv, emb_demo=None):\n",
    "        # q: (b, query_len, d_model)\n",
    "        # kv: (b, visit_len, d_model)\n",
    "        # emb_demo: (b, d_demo)\n",
    "        \n",
    "        output, attn = self.multihead_attn(q, kv, kv)\n",
    "        output = output.view(q.size(0), -1) # output: (b, d_model * 2)\n",
    "        \n",
    "        if emb_demo is not None:\n",
    "            assert emb_demo.size(-1) == self.d_demo # model sanity\n",
    "            \n",
    "            output = torch.cat([output, emb_demo], dim=1)\n",
    "\n",
    "        output = self.linear(output) # (b, d_out)\n",
    "        \n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfe3e410-2bb8-4e06-ac49-70be10e4a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HealthExpenditurePredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Health Expenditures Prediction Module\n",
    "    \n",
    "    Args:\n",
    "        n_disease (int): the number of prediction diseases\n",
    "        d_out (int): prediction output dimension\n",
    "        d_visit (int): visit embedding dimension\n",
    "        d_query (int): combined query dimension\n",
    "        d_demo (int): demographic embedding dimesion if it exists, or 0\n",
    "        d_model (int): (multihead attention) model dimension\n",
    "        n_head (int): the number of (multihead attention) heads\n",
    "        dropout (float): dropout ratio of a multihead attention and a final linear layer of HealthExpenditureModule\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, n_disease, d_out, d_visit, d_query, d_demo=0, d_model=512, n_head=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.module = HealthExpenditureModule(d_out, d_demo, d_model, n_head, dropout)\n",
    "        \n",
    "        # Embedding layers for query, visit, demo\n",
    "        self.w_q1 = nn.Embedding(n_disease, d_query) # for sparse input data, such as one-hot encoded vectors\n",
    "        self.w_q2 = nn.Linear(1, d_query, bias=False)\n",
    "        self.w_qs = nn.Linear(d_query, d_model, bias=False)\n",
    "        self.w_visit = nn.Linear(d_visit, d_model)\n",
    "        if d_demo != 0:\n",
    "            self.w_demo = nn.Linear(d_demo, d_demo, bias=False)\n",
    "    \n",
    "    def forward(self, q_1, q_2, emb_visit, emb_demo=None):\n",
    "        # q_1 (torch.LongTensor): (b)\n",
    "        # q_2: (b, 1)\n",
    "        \n",
    "        q_1 = self.w_q1(q_1) # q_1: (b, d_query)\n",
    "        q_2 = self.w_q2(q_2) # q_2: (b, d_query)\n",
    "        \n",
    "        # Concatenating queries\n",
    "        q = torch.stack((q_1, q_2), dim=1) # q: (b, query_len, d_query)\n",
    "        \n",
    "        # Query\n",
    "        q = self.w_qs(q) # q: (b, query_len, d_model)\n",
    "        \n",
    "        # Key, Value\n",
    "        kv = self.w_visit(emb_visit) # kv: (b, visit_len, d_model)\n",
    "        \n",
    "        # Concatenating demographic embeddings\n",
    "        if emb_demo is not None:\n",
    "            emb_demo = self.w_demo(emb_demo) # emb_demo: (b, d_demo)\n",
    "        \n",
    "        output, attn = self.module(q, kv, emb_demo)\n",
    "        \n",
    "        return output, attn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14f8a45f",
   "metadata": {},
   "source": [
    "# Demo 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "edf15391-6c0e-4a91-a0b6-bfeea3411af9",
   "metadata": {},
   "source": [
    "## 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1de4ffd-6018-40c9-9170-c02b82e2c549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query1:  tensor([4, 1, 5, 0, 2, 0, 3])\n",
      "query2:  tensor([[-0.0722],\n",
      "        [-0.3962],\n",
      "        [-0.2648],\n",
      "        [-0.6019],\n",
      "        [-1.4203],\n",
      "        [ 2.1846],\n",
      "        [-1.0118]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "질병, 질병심각도 (Q)\n",
    "\"\"\"\n",
    "\n",
    "# Six diseases: A, B, C, D, E, F\n",
    "A = torch.LongTensor([0])\n",
    "B = torch.LongTensor([1])\n",
    "C = torch.LongTensor([2])\n",
    "D = torch.LongTensor([3])\n",
    "E = torch.LongTensor([4])\n",
    "F = torch.LongTensor([5])\n",
    "\n",
    "# Seven patients\n",
    "query1 = torch.cat((E, B, F, A, C, A, D))\n",
    "print('query1: ', query1)\n",
    "\n",
    "# Severity level: (from a normal distribution)\n",
    "query2 = torch.randn(7,1)\n",
    "print('query2: ', query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aeb39a8-bfa5-436a-a6aa-dd0026ec4c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "방문표현 (K,V)\n",
    "\"\"\"\n",
    "\n",
    "# Random visit embedding\n",
    "n_patients = 7\n",
    "T = 14\n",
    "visit_embed_size = 5\n",
    "x_v = torch.randn([n_patients, T, visit_embed_size]) # batch_first\n",
    "\n",
    "\"\"\"\n",
    "demographic emb\n",
    "\"\"\"\n",
    "# Random demographic embedding\n",
    "demo_embed_size = 9\n",
    "x_d = torch.randn([n_patients, demo_embed_size])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "acc92ae1-e0db-43d5-a0e4-d511440e2787",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e604e003-7fd0-4970-91ff-f1270b8a44f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "num_disease = 6\n",
    "dim_output = 3 # Three expenditures: total, patient, NHIS\n",
    "dim_visit = 5\n",
    "dim_query = 5\n",
    "dim_demo = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc74d71d-8570-4b9e-9c18-c999f3534152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HealthExpenditurePredictor(\n",
       "  (module): HealthExpenditureModule(\n",
       "    (multihead_attn): MultiHeadAttention(\n",
       "      (w_qs): Linear(in_features=512, out_features=512, bias=False)\n",
       "      (w_ks): Linear(in_features=512, out_features=512, bias=False)\n",
       "      (w_vs): Linear(in_features=512, out_features=512, bias=False)\n",
       "      (fc): Linear(in_features=512, out_features=512, bias=False)\n",
       "      (attention): ScaledDotProductAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=1033, out_features=516, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): Linear(in_features=516, out_features=3, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (w_q1): Embedding(6, 5)\n",
       "  (w_q2): Linear(in_features=1, out_features=5, bias=False)\n",
       "  (w_qs): Linear(in_features=5, out_features=512, bias=False)\n",
       "  (w_visit): Linear(in_features=5, out_features=512, bias=True)\n",
       "  (w_demo): Linear(in_features=9, out_features=9, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hep = HealthExpenditurePredictor(num_disease, dim_output, dim_visit, dim_query, d_demo=dim_demo)\n",
    "hep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8537ddff-0b53-416e-9aa9-d417a122a820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0630,  0.2212,  0.0496],\n",
       "        [-0.1097,  0.0945,  0.1078],\n",
       "        [-0.1006,  0.0935,  0.0080],\n",
       "        [-0.1850, -0.1710,  0.0608],\n",
       "        [ 0.0460,  0.0794,  0.0278],\n",
       "        [-0.0991, -0.0287,  0.1441],\n",
       "        [ 0.0513,  0.1180, -0.0397]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hep_output, hep_attn = hep(query1, query2, x_v, x_d)\n",
    "\n",
    "print(hep_output.size())\n",
    "hep_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc959d9b",
   "metadata": {},
   "source": [
    "# Demo 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ca1b7e0",
   "metadata": {},
   "source": [
    "## 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfee16d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query1:  tensor([2, 3, 4,  ..., 1, 2, 0])\n",
      "query2:  tensor([[ 0.5559],\n",
      "        [-2.7790],\n",
      "        [ 0.4456],\n",
      "        ...,\n",
      "        [-0.9780],\n",
      "        [ 1.3913],\n",
      "        [-0.1570]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "질병, 질병심각도 (Q)\n",
    "\"\"\"\n",
    "\n",
    "# Six diseases: A, B, C, D, E, F\n",
    "A = torch.LongTensor([0])\n",
    "B = torch.LongTensor([1])\n",
    "C = torch.LongTensor([2])\n",
    "D = torch.LongTensor([3])\n",
    "E = torch.LongTensor([4])\n",
    "F = torch.LongTensor([5])\n",
    "\n",
    "# 10000 patients\n",
    "# create a tuple of 10000 elements, by randomly sampling from the above 6 diseases\n",
    "query1 = torch.cat([torch.LongTensor([random.randint(0,5)]) for _ in range(10000)])\n",
    "print('query1: ', query1)\n",
    "\n",
    "# Severity level: (from a normal distribution)\n",
    "query2 = torch.randn(10000,1)\n",
    "print('query2: ', query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcd2fb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "방문표현 (K,V)\n",
    "\"\"\"\n",
    "\n",
    "# Random visit embedding\n",
    "n_patients = 10000\n",
    "T = 14\n",
    "visit_embed_size = 5\n",
    "x_v = torch.randn([n_patients, T, visit_embed_size]) # batch_first\n",
    "\n",
    "\"\"\"\n",
    "demographic emb\n",
    "\"\"\"\n",
    "# Random demographic embedding\n",
    "demo_embed_size = 9\n",
    "x_d = torch.randn([n_patients, demo_embed_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c6a356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "치료비 (Y)\n",
    "\"\"\"\n",
    "\n",
    "# Random health expenditure\n",
    "y = torch.randn([n_patients, 3]) # torch.Size([10000, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a09fec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset\n",
    "\"\"\"\n",
    "# create a dataloader for the following data: query1, query2, x_v, x_d, y\n",
    "# total number of data points = 10000\n",
    "# batch size = 64\n",
    "\n",
    "class MyDataset(data.Dataset):\n",
    "    def __init__(self, query1, query2, x_v, x_d, y):\n",
    "        self.query1 = query1\n",
    "        self.query2 = query2\n",
    "        self.x_v = x_v\n",
    "        self.x_d = x_d\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.query1[index], self.query2[index], self.x_v[index], self.x_d[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.query1)\n",
    "    \n",
    "dataset = MyDataset(query1, query2, x_v, x_d, y)\n",
    "\n",
    "\"\"\"\n",
    "Data split\n",
    "\"\"\"\n",
    "\n",
    "# split the data into train, validation, test\n",
    "# train: 80%\n",
    "# validation: 10%\n",
    "# test: 10%\n",
    "\n",
    "train_size = math.floor(0.8 * len(dataset))\n",
    "valid_size = math.floor(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - valid_size\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size, test_size])\n",
    "\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_dataloader = data.DataLoader(valid_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = data.DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f06d2550",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "571a3220",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get model\n",
    "\"\"\"\n",
    "\n",
    "# hyperparameters\n",
    "num_disease = 6\n",
    "dim_output = 3 # Three expenditures: total, patient, NHIS\n",
    "dim_visit = 5\n",
    "dim_query = 5\n",
    "dim_demo = 9\n",
    "\n",
    "model = HealthExpenditurePredictor(num_disease, dim_output, dim_visit, dim_query, d_demo=dim_demo)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ef80b59",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fd454311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.010723 \tValidation Loss: 0.974676 \tTime: 6.11 sec\n",
      "Validation loss decreased (inf --> 0.974676).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.010812 \tValidation Loss: 0.976384 \tTime: 5.98 sec\n",
      "Epoch: 3 \tTraining Loss: 1.010791 \tValidation Loss: 0.978181 \tTime: 5.94 sec\n",
      "Epoch: 4 \tTraining Loss: 1.010724 \tValidation Loss: 0.979848 \tTime: 5.93 sec\n",
      "Epoch: 5 \tTraining Loss: 1.010738 \tValidation Loss: 0.977986 \tTime: 5.96 sec\n",
      "Epoch: 6 \tTraining Loss: 1.010725 \tValidation Loss: 0.979549 \tTime: 5.94 sec\n",
      "Epoch: 7 \tTraining Loss: 1.010707 \tValidation Loss: 0.982019 \tTime: 5.94 sec\n",
      "Epoch: 8 \tTraining Loss: 1.010734 \tValidation Loss: 0.982883 \tTime: 5.92 sec\n",
      "Epoch: 9 \tTraining Loss: 1.010743 \tValidation Loss: 0.979857 \tTime: 6.01 sec\n",
      "Epoch: 10 \tTraining Loss: 1.010715 \tValidation Loss: 0.982338 \tTime: 5.95 sec\n",
      "Epoch: 11 \tTraining Loss: 1.010709 \tValidation Loss: 0.978592 \tTime: 5.95 sec\n",
      "Epoch: 12 \tTraining Loss: 1.010711 \tValidation Loss: 0.985971 \tTime: 5.95 sec\n",
      "Epoch: 13 \tTraining Loss: 1.010690 \tValidation Loss: 0.977832 \tTime: 5.95 sec\n",
      "Epoch: 14 \tTraining Loss: 1.010702 \tValidation Loss: 0.977763 \tTime: 5.97 sec\n",
      "Epoch: 15 \tTraining Loss: 1.010711 \tValidation Loss: 0.983075 \tTime: 5.95 sec\n",
      "Epoch: 16 \tTraining Loss: 1.010737 \tValidation Loss: 0.978605 \tTime: 5.96 sec\n",
      "Epoch: 17 \tTraining Loss: 1.010709 \tValidation Loss: 0.975718 \tTime: 6.09 sec\n",
      "Epoch: 18 \tTraining Loss: 1.010738 \tValidation Loss: 0.982502 \tTime: 6.02 sec\n",
      "Epoch: 19 \tTraining Loss: 1.010726 \tValidation Loss: 0.976297 \tTime: 5.97 sec\n",
      "Epoch: 20 \tTraining Loss: 1.010757 \tValidation Loss: 0.977470 \tTime: 5.96 sec\n",
      "Epoch: 21 \tTraining Loss: 1.010715 \tValidation Loss: 0.975887 \tTime: 5.97 sec\n",
      "Epoch: 22 \tTraining Loss: 1.010726 \tValidation Loss: 0.974142 \tTime: 5.99 sec\n",
      "Validation loss decreased (0.974676 --> 0.974142).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 1.010707 \tValidation Loss: 0.979656 \tTime: 5.98 sec\n",
      "Epoch: 24 \tTraining Loss: 1.010723 \tValidation Loss: 0.977433 \tTime: 5.94 sec\n",
      "Epoch: 25 \tTraining Loss: 1.010711 \tValidation Loss: 0.980025 \tTime: 5.95 sec\n",
      "Epoch: 26 \tTraining Loss: 1.010754 \tValidation Loss: 0.982172 \tTime: 5.97 sec\n",
      "Epoch: 27 \tTraining Loss: 1.010710 \tValidation Loss: 0.976078 \tTime: 5.97 sec\n",
      "Epoch: 28 \tTraining Loss: 1.010755 \tValidation Loss: 0.976259 \tTime: 5.95 sec\n",
      "Epoch: 29 \tTraining Loss: 1.010726 \tValidation Loss: 0.983420 \tTime: 5.95 sec\n",
      "Epoch: 30 \tTraining Loss: 1.010716 \tValidation Loss: 0.983726 \tTime: 5.96 sec\n",
      "Epoch: 31 \tTraining Loss: 1.010714 \tValidation Loss: 0.978914 \tTime: 5.97 sec\n",
      "Epoch: 32 \tTraining Loss: 1.010708 \tValidation Loss: 0.985086 \tTime: 6.00 sec\n",
      "Epoch: 33 \tTraining Loss: 1.010758 \tValidation Loss: 0.979011 \tTime: 5.95 sec\n",
      "Epoch: 34 \tTraining Loss: 1.010715 \tValidation Loss: 0.976834 \tTime: 5.97 sec\n",
      "Epoch: 35 \tTraining Loss: 1.010709 \tValidation Loss: 0.978898 \tTime: 6.03 sec\n",
      "Epoch: 36 \tTraining Loss: 1.010711 \tValidation Loss: 0.977026 \tTime: 6.01 sec\n",
      "Epoch: 37 \tTraining Loss: 1.010747 \tValidation Loss: 0.976948 \tTime: 5.97 sec\n",
      "Epoch: 38 \tTraining Loss: 1.010705 \tValidation Loss: 0.979287 \tTime: 5.98 sec\n",
      "Epoch: 39 \tTraining Loss: 1.010761 \tValidation Loss: 0.982319 \tTime: 5.96 sec\n",
      "Epoch: 40 \tTraining Loss: 1.010714 \tValidation Loss: 0.976102 \tTime: 5.96 sec\n",
      "Epoch: 41 \tTraining Loss: 1.010716 \tValidation Loss: 0.976315 \tTime: 6.06 sec\n",
      "Epoch: 42 \tTraining Loss: 1.010766 \tValidation Loss: 0.978629 \tTime: 6.07 sec\n",
      "Epoch: 43 \tTraining Loss: 1.010747 \tValidation Loss: 0.981700 \tTime: 5.97 sec\n",
      "Epoch: 44 \tTraining Loss: 1.010734 \tValidation Loss: 0.978657 \tTime: 5.98 sec\n",
      "Epoch: 45 \tTraining Loss: 1.010731 \tValidation Loss: 0.983577 \tTime: 5.96 sec\n",
      "Epoch: 46 \tTraining Loss: 1.010724 \tValidation Loss: 0.977320 \tTime: 5.99 sec\n",
      "Epoch: 47 \tTraining Loss: 1.010753 \tValidation Loss: 0.979657 \tTime: 5.97 sec\n",
      "Epoch: 48 \tTraining Loss: 1.010752 \tValidation Loss: 0.977003 \tTime: 5.98 sec\n",
      "Epoch: 49 \tTraining Loss: 1.010717 \tValidation Loss: 0.984483 \tTime: 5.96 sec\n",
      "Epoch: 50 \tTraining Loss: 1.010720 \tValidation Loss: 0.976421 \tTime: 5.97 sec\n",
      "Epoch: 51 \tTraining Loss: 1.010734 \tValidation Loss: 0.979823 \tTime: 5.97 sec\n",
      "Epoch: 52 \tTraining Loss: 1.010723 \tValidation Loss: 0.978852 \tTime: 6.00 sec\n",
      "Epoch: 53 \tTraining Loss: 1.010725 \tValidation Loss: 0.980150 \tTime: 6.12 sec\n",
      "Epoch: 54 \tTraining Loss: 1.010718 \tValidation Loss: 0.980544 \tTime: 6.05 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m output, _ \u001b[39m=\u001b[39m model(query1, query2, x_v, x_d)\n\u001b[0;32m     27\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, y)\n\u001b[1;32m---> 28\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     29\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     30\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\lesga\\anaconda3\\envs\\Insurance\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    490\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\lesga\\anaconda3\\envs\\Insurance\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train model on train_dataloader\n",
    "\"\"\"\n",
    "\n",
    "# loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# number of epochs\n",
    "n_epochs = 100\n",
    "\n",
    "# training: save the model with the lowest validation loss\n",
    "valid_loss_min = np.Inf\n",
    "for epoch in range(n_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    # train the model\n",
    "    model.train()\n",
    "    for batch_idx, (query1, query2, x_v, x_d, y) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(query1, query2, x_v, x_d)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    # validate the model\n",
    "    model.eval()\n",
    "    for batch_idx, (query1, query2, x_v, x_d, y) in enumerate(valid_dataloader):\n",
    "        output, _ = model(query1, query2, x_v, x_d)\n",
    "        loss = criterion(output, y)\n",
    "        valid_loss += loss.item()\n",
    "        \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_dataloader)\n",
    "    valid_loss = valid_loss/len(valid_dataloader)\n",
    "    \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tTime: {:.2f} sec'.format(\n",
    "        epoch+1, train_loss, valid_loss, time.time() - start_time))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "        valid_loss_min = valid_loss\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63710b5a",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "aa247260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.011417\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test model on test_dataloader\n",
    "\"\"\"\n",
    "\n",
    "# load the model that got the best validation loss\n",
    "model.load_state_dict(torch.load('model.pt'))\n",
    "\n",
    "# get test loss\n",
    "test_loss = 0.0\n",
    "model.eval()\n",
    "for batch_idx, (query1, query2, x_v, x_d, y) in enumerate(test_dataloader):\n",
    "    output, _ = model(query1, query2, x_v, x_d)\n",
    "    loss = criterion(output, y)\n",
    "    test_loss += loss.item()\n",
    "\n",
    "# calculate and print avg test loss\n",
    "test_loss = test_loss/len(test_dataloader)\n",
    "\n",
    "print('Test Loss: {:.6f}'.format(test_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "299943d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eac14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class HealthExpenditureModule(object):\n",
    "    \"\"\"\n",
    "    Health Expenditures Module\n",
    "    \n",
    "    Args:\n",
    "        d_out (int): prediction output dimension\n",
    "        d_demo (int): demographic embedding dimesion if it exists, or 0\n",
    "        d_model (int): (multihead attention) model dimension\n",
    "        n_head (int): the number of (multihead attention) heads\n",
    "        dropout (float): dropout ratio of a multihead attention and a final linear layer\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, d_out, d_demo=0, d_model=512, n_head=8, dropout=0.1):\n",
    "        self.d_demo = d_demo\n",
    "        \n",
    "        assert d_model % n_head == 0\n",
    "        self.multihead_attn = MultiHeadAttention(n_head, d_model, d_k = (d_model // n_head), d_v = (d_model // n_head), dropout=dropout)\n",
    "        \n",
    "        d_inter = d_model * 2 + d_demo\n",
    "        \n",
    "        self.linear = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(d_inter // 2),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dropout(dropout),\n",
    "            tf.keras.layers.Dense(d_out)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, q, kv, emb_demo=None):\n",
    "        # q: (b, query_len, d_model)\n",
    "        # kv: (b, visit_len, d_model)\n",
    "        # emb_demo: (b, d_demo)\n",
    "        \n",
    "        output, attn = self.multihead_attn(q, kv, kv)\n",
    "        output = tf.reshape(output, [tf.shape(q)[0], -1]) # output: (b, d_model * 2)\n",
    "        \n",
    "        if emb_demo is not None:\n",
    "            assert emb_demo.get_shape().as_list()[-1] == self.d_demo # model sanity\n",
    "            \n",
    "            output = tf.concat([output, emb_demo], axis=-1)\n",
    "\n",
    "        output = self.linear(output) # (b, d_out)\n",
    "        \n",
    "        return output, attn\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Insurance_TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "099f8f533c2a88581e5040e2f5281b2ce78f44c53a88f545b532a21a855e1d1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
